import re
import torch
from torch.nn.utils.rnn import pad_sequence
from collections import Counter
from nltk.tokenize import word_tokenize
import nltk
nltk.download('punkt')

# 1. Load a small dataset
text = """
Once upon a time, there was a little robot who dreamed of writing stories.
Every night, it would read books and try to generate its own tales.
Some were funny, others were wise, and all were unique in their own way.
"""

# 2. Clean the dataset (lowercase + remove punctuation)
def clean_text(text):
    text = text.lower()
    text = re.sub(r"[^a-zA-Z0-9\s]", "", text)
    return text

cleaned_text = clean_text(text)

# 3. Tokenize into words
tokens = word_tokenize(cleaned_text)

# 4. Build vocabulary
vocab = Counter(tokens)
vocab = {word: idx + 2 for idx, (word, _) in enumerate(vocab.most_common())}
vocab["<PAD>"] = 0
vocab["<UNK>"] = 1

# 5. Encode tokens
encoded = [vocab.get(word, vocab["<UNK>"]) for word in tokens]

# 6. Create fixed-length sequences and attention masks
SEQ_LEN = 10
sequences = []
for i in range(len(encoded) - SEQ_LEN):
    seq = encoded[i:i+SEQ_LEN]
    sequences.append(torch.tensor(seq))

# Pad sequences (if needed)
padded_sequences = pad_sequence(sequences, batch_first=True, padding_value=vocab["<PAD>"])
attention_masks = (padded_sequences != vocab["<PAD>"]).int()

print("Sample sequence:", padded_sequences[0])
print("Attention mask: ", attention_masks[0])
print("Vocab size: ", len(vocab))
